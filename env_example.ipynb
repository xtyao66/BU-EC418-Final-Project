{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the environment\n",
    "These are the packages needed for running the env. You may be required to install additional packages to support them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PySuperTuxKart\n",
    "\n",
    "%pip install imageio\n",
    "\n",
    "%pip install tensorboard\n",
    "\n",
    "%pip install stable-baselines3[extra]\n",
    "\n",
    "%pip install gym\n",
    "\n",
    "%pip install moviepy\n",
    "\n",
    "#replace with proper version of torch, torchvision and torchaudio\n",
    "# %pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu124\n",
    "\n",
    "\n",
    "# %pip install sb3-contrib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This codes checks if cuda is abailable. If it is, then you have successfully installed pytorch with cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the GPU is available\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's creat the environment. The imported Env is a GYM env, it needs to be converted to a venv. In theory, vector envs can be crated multiple times and trained in parallel. But in this case, our env can only be created once. So we are using the dummy DummyVecEnv to create a vector with just one instance.\n",
    "\n",
    "This shouldn't require any change. The last line checks if the env is valid. It should run without any error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: you shouldn't run this cell twice as only one environment can be created at a time\n",
    "# if you want to run this cell again, you need to restart the runtime\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO, A2C, DQN, SAC, TD3\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecNormalize, VecFrameStack, VecVideoRecorder, DummyVecEnv, VecMonitor\n",
    "\n",
    "# Add the path to the custom environment\n",
    "sys.path.insert(0, \"homework\")\n",
    "\n",
    "import kartEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "from gymnasium.envs.registration import make, register, registry, spec\n",
    "\n",
    "# This makes sure that the custom environment is registered only once\n",
    "if \"kartEnv-v0\" not in registry:\n",
    "\n",
    "    # Register the custom environment as kartEnv-v0\n",
    "    register(\n",
    "        id=\"kartEnv-v0\",\n",
    "        entry_point=\"kartEnv:kartEnv\",\n",
    "        max_episode_steps=2000\n",
    "    )\n",
    "\n",
    "    # Now you can make the environment\n",
    "    env = gym.make(\"kartEnv-v0\")\n",
    "    # Now you have a gym environment\n",
    "\n",
    "# Check if the environment is valid, if not, fix it\n",
    "check_env(env)\n",
    "\n",
    "# DO NOT overwrite the env variable, it will break the environment,\n",
    "# and we will use env later\n",
    "\n",
    "# Convert the environment to a vectorized environment, useful for using SB3 functions like VecNormalize\n",
    "venv = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Wrap the environment with a monitor, useful for logging to tensorboard\n",
    "venv = VecMonitor(venv)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize and normalize the environment\n",
    "This cell includes a notmalization to the environment.\n",
    "\n",
    "SB3 documents suggests that normalization can help with the performance of the model.\n",
    "You can uncomment the lines and change the True/ False to see the difference.\n",
    "\n",
    "## loading the normalized environment\n",
    "This code can load a previously used venv normalization parameters. I will show you how to save the params later.\n",
    "\n",
    "Don't from saved unless you have exact same environment as the one you saved.\n",
    "Otherwise, you may experiecne stability issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_checkpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the environment, may be useful for training\n",
    "# might be helpful to load the VecNormalize object from a file if it already exists\n",
    "if (load_from_checkpoint and os.patorch.exists(\"venv.pkl\")):\n",
    "    venv = VecNormalize.load(\"venv.pkl\", venv)\n",
    "else:\n",
    "    venv = VecNormalize(venv, \n",
    "                training=True, \n",
    "                norm_obs=True,                 \n",
    "                norm_obs_keys= [\"speed\"],\n",
    "                norm_reward=True, \n",
    "                clip_obs=5,\n",
    "                clip_reward=100.0, \n",
    "                gamma=0.99, \n",
    "                epsilon=1e-08,\n",
    "\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see an image of kart on track without any error, it means the venv is set up properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_obs = venv.reset()\n",
    "print(sample_obs)\n",
    "print(sample_obs[\"speed\"]) # print a sample of the observation, also show the image of the track as an image\n",
    "print(\"observation shapes: \", sample_obs[\"speed\"].shape, sample_obs[\"image\"].shape)\n",
    "plt.imshow(np.moveaxis(sample_obs[\"image\"].squeeze(), 0, -1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model\n",
    "The code below shows an example of using Stable Baselines 3 PPO. It also shows how to define custom feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from typing import Callable\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "n_input_channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature extractor is essentially a CNN that applies to the image and a MLP applies to the speed. \n",
    "\n",
    "After extraction, the result is passes to the policy and value MLP.\n",
    "\n",
    "There are different options to define the feature extractor. A simple CNN example is shown. You can adjust it as you need.\n",
    "\n",
    "Check this, https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCombinedExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Dict):\n",
    "        \"\"\"\n",
    "        Initialize the custom feature extractor.\n",
    "\n",
    "        :param observation_space: The observation space of the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        extractors = {}\n",
    "        total_concat_size = 0\n",
    "\n",
    "        for key, subspace in observation_space.items():\n",
    "\n",
    "            # CNN for image input feature extraction\n",
    "            if key == \"image\":\n",
    "                \n",
    "                # Option 1: Use a custom CNN architecture\n",
    "\n",
    "                extractors[key] = nn.Sequential(\n",
    "\n",
    "                    # conv block 1\n",
    "                    nn.Conv2d(n_input_channels, 64, kernel_size=8, stride=3, padding=3),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=2),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "                    \n",
    "\n",
    "                    # conv block 2\n",
    "                    nn.Conv2d(64, 128, kernel_size=6, stride=2, padding=2),\n",
    "                    nn.BatchNorm2d(128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=2),\n",
    "                    nn.BatchNorm2d(128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "\n",
    "\n",
    "                    # conv block 3\n",
    "                    nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2),\n",
    "                    nn.BatchNorm2d(256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(256, 256, kernel_size=4, stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "\n",
    "                    # Calculate the output size after the CNN\n",
    "                    # This is needed to create the linear layer\n",
    "                    \n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(1536, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p=0.5),\n",
    "                )   \n",
    "\n",
    "                total_concat_size += 256\n",
    "\n",
    "            # MLP for speed input feature extraction\n",
    "            elif key == \"speed\":\n",
    "                # Simple MLP for speed input\n",
    "                extractors[key] = nn.Sequential(\n",
    "                    nn.Linear(subspace.shape[0], 64),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "                total_concat_size += 64\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Extractor for key '{key}' is not implemented.\")\n",
    "\n",
    "        # Now, initialize the parent class with the correct features_dim\n",
    "        super(CustomCombinedExtractor, self).__init__(observation_space, features_dim=total_concat_size)\n",
    "\n",
    "        self.extractors = nn.ModuleDict(extractors)\n",
    "\n",
    "\n",
    "    def forward(self, observations: dict) -> torch.Tensor:\n",
    "        \n",
    "        \"\"\"\n",
    "        Forward pass through the feature extractor.\n",
    "\n",
    "        :param observations: Dictionary of observations.\n",
    "        :return: Concatenated feature tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        encoded_tensor_list = []\n",
    "        for key, extractor in self.extractors.items():\n",
    "            obs = observations[key]\n",
    "            obs = torch.tensor(obs, dtype=torch.float32) if not isinstance(obs, torch.Tensor) else obs\n",
    "            encoded_tensor_list.append(extractor(obs))\n",
    "\n",
    "        return torch.cat(encoded_tensor_list, dim=1)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative design can be residual CNN, or preimplemnted networks by pytorch. Examples shown as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, stride=1):\n",
    "#         super(ResidualBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "#         self.downsample = nn.Sequential()\n",
    "#         if stride != 1 or in_channels != out_channels:\n",
    "#             self.downsample = nn.Sequential(\n",
    "#                 nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "#                 nn.BatchNorm2d(out_channels)\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         identity = self.downsample(x)\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#         out += identity\n",
    "#         out = self.relu(out)\n",
    "#         return out\n",
    "\n",
    "# # # Custom feature extractor for environments with image and speed inputs\n",
    "# class CustomCombinedExtractor(BaseFeaturesExtractor):\n",
    "#     def __init__(self, observation_space: gym.spaces.Dict):\n",
    "#         \"\"\"\n",
    "#         Initialize the custom feature extractor.\n",
    "\n",
    "#         :param observation_space: The observation space of the environment.\n",
    "#         \"\"\"\n",
    "\n",
    "#         extractors = {}\n",
    "#         total_concat_size = 0\n",
    "\n",
    "#         for key, subspace in observation_space.items():\n",
    "#             if key == \"image\":\n",
    "                \n",
    "#                 # Option 2: Use a similar architecture to ResNet, but with less layers\n",
    "\n",
    "#                 # Define a more complex CNN architecture for better feature extraction\n",
    "#                 extractors[key] = nn.Sequential(\n",
    "#                     nn.Conv2d(n_input_channels, 64, kernel_size=7, stride=2, padding=3),\n",
    "#                     nn.BatchNorm2d(64),\n",
    "#                     nn.ReLU(),\n",
    "#                     nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                    \n",
    "#                     # Residual blocks, the number of blocks can be increased for better performance, but it will increase the training time\n",
    "#                     ResidualBlock(64, 64),\n",
    "#                     ResidualBlock(64, 64),\n",
    "\n",
    "#                     ResidualBlock(64, 128, stride=2),\n",
    "#                     ResidualBlock(128, 128),\n",
    "#                     ResidualBlock(128, 128),\n",
    "\n",
    "#                     ResidualBlock(128, 256, stride=2),\n",
    "#                     ResidualBlock(256, 256),\n",
    "#                     ResidualBlock(256, 256),\n",
    "#                     ResidualBlock(256, 256),\n",
    "\n",
    "#                     ResidualBlock(256, 512, stride=2),\n",
    "#                     ResidualBlock(512, 512),\n",
    "\n",
    "#                     nn.AdaptiveAvgPool2d((1, 1)),\n",
    "\n",
    "#                     nn.Flatten(),\n",
    "#                     # Calculate the output size after the CNN\n",
    "#                     # Assuming input image size is (3, 96, 128)\n",
    "#                     # After Conv layers:\n",
    "#                     # (96, 128) -> (64, 48, 64) -> (64, 24, 32) -> (128, 24, 32) -> (128, 12, 16)\n",
    "#                     # -> (256, 12, 16) -> (256, 6, 8) -> (512, 6, 8) -> (512, 3, 4) -> (512, 1, 1)  \n",
    "                    \n",
    "#                     nn.Linear(512, 256),\n",
    "#                     nn.ReLU(),\n",
    "#                     nn.Dropout(p=0.5),\n",
    "#                 )\n",
    "\n",
    "#                 total_concat_size += 256\n",
    "\n",
    "\n",
    "#             elif key == \"speed\":\n",
    "#                 # Simple MLP for speed input\n",
    "#                 extractors[key] = nn.Sequential(\n",
    "#                     nn.Linear(subspace.shape[0], 32),\n",
    "#                     nn.ReLU(),\n",
    "#                 )\n",
    "#                 total_concat_size += 32\n",
    "#             else:\n",
    "#                 raise NotImplementedError(f\"Extractor for key '{key}' is not implemented.\")\n",
    "\n",
    "#         # Now, initialize the parent class with the correct features_dim\n",
    "#         super(CustomCombinedExtractor, self).__init__(observation_space, features_dim=total_concat_size)\n",
    "\n",
    "#         self.extractors = nn.ModuleDict(extractors)\n",
    "\n",
    "#     def forward(self, observations: dict) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Forward pass through the feature extractor.\n",
    "\n",
    "#         :param observations: Dictionary of observations.\n",
    "#         :return: Concatenated feature tensor.\n",
    "#         \"\"\"\n",
    "#         encoded_tensor_list = []\n",
    "#         for key, extractor in self.extractors.items():\n",
    "#             obs = observations[key]\n",
    "#             obs = torch.tensor(obs, dtype=torch.float32) if not isinstance(obs, torch.Tensor) else obs\n",
    "\n",
    "#             if key == \"image\":\n",
    "#                 # Ensure the image is of type float and normalized\n",
    "#                 obs = obs.float() / 255.0\n",
    "            \n",
    "#             encoded_tensor_list.append(extractor(obs))\n",
    "#         return torch.cat(encoded_tensor_list, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, stride=1):\n",
    "#         super(ResidualBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "#         self.downsample = nn.Sequential()\n",
    "#         if stride != 1 or in_channels != out_channels:\n",
    "#             self.downsample = nn.Sequential(\n",
    "#                 nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "#                 nn.BatchNorm2d(out_channels)\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         identity = self.downsample(x)\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#         out += identity\n",
    "#         out = self.relu(out)\n",
    "#         return out\n",
    "\n",
    "# # # Custom feature extractor for environments with image and speed inputs\n",
    "# class CustomCombinedExtractor(BaseFeaturesExtractor):\n",
    "#     def __init__(self, observation_space: gym.spaces.Dict):\n",
    "#         \"\"\"\n",
    "#         Initialize the custom feature extractor.\n",
    "\n",
    "#         :param observation_space: The observation space of the environment.\n",
    "#         \"\"\"\n",
    "\n",
    "#         extractors = {}\n",
    "#         total_concat_size = 0\n",
    "\n",
    "#         for key, subspace in observation_space.items():\n",
    "#             if key == \"image\":\n",
    "\n",
    "#                 # Option 3: Use a ResNeXt model\n",
    "#                 # Load a ResNeXt model\n",
    "\n",
    "#                 resnext = models.resnext50_32x4d(pretrained=False)\n",
    "\n",
    "#                 # Remove the fully connected layer and avgpool\n",
    "#                 modules = list(resnext.children())[:-2]  # Exclude avgpool and fc\n",
    "#                 resnext = nn.Sequential(*modules)\n",
    "\n",
    "#                 # Add AdaptiveAvgPool and Flatten\n",
    "#                 resnext = nn.Sequential(\n",
    "#                     resnext,\n",
    "#                     nn.AdaptiveAvgPool2d((1, 1)),\n",
    "#                     nn.Flatten(),\n",
    "#                     nn.Linear(2048, 256),  # ResNeXt-50-32x4d has 2048 output channels\n",
    "#                     nn.ReLU(),\n",
    "#                     nn.Dropout(p=0.5),\n",
    "#                 )\n",
    "\n",
    "#                 # Optionally, freeze ResNeXt layers to prevent training\n",
    "#                 for param in resnext.parameters():\n",
    "#                     param.requires_grad = False\n",
    "\n",
    "#                 extractors[key] = resnext\n",
    "#                 total_concat_size += 256  # Size after Linear layer\n",
    "\n",
    "#             elif key == \"speed\":\n",
    "#                 # Simple MLP for speed input\n",
    "#                 extractors[key] = nn.Sequential(\n",
    "#                     nn.Linear(subspace.shape[0], 32),\n",
    "#                     nn.ReLU(),\n",
    "#                 )\n",
    "#                 total_concat_size += 32\n",
    "#             else:\n",
    "#                 raise NotImplementedError(f\"Extractor for key '{key}' is not implemented.\")\n",
    "\n",
    "#         # Now, initialize the parent class with the correct features_dim\n",
    "#         super(CustomCombinedExtractor, self).__init__(observation_space, features_dim=total_concat_size)\n",
    "\n",
    "#         self.extractors = nn.ModuleDict(extractors)\n",
    "\n",
    "#     def forward(self, observations: dict) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Forward pass through the feature extractor.\n",
    "\n",
    "#         :param observations: Dictionary of observations.\n",
    "#         :return: Concatenated feature tensor.\n",
    "#         \"\"\"\n",
    "#         encoded_tensor_list = []\n",
    "#         for key, extractor in self.extractors.items():\n",
    "#             obs = observations[key]\n",
    "#             obs = torch.tensor(obs, dtype=torch.float32) if not isinstance(obs, torch.Tensor) else obs\n",
    "\n",
    "#             if key == \"image\":\n",
    "#                 # Ensure the image is of type float and normalized\n",
    "#                 obs = obs.float() / 255.0\n",
    "            \n",
    "#             encoded_tensor_list.append(extractor(obs))\n",
    "#         return torch.cat(encoded_tensor_list, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell below should run without error. It checks the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sample_observation = venv.reset()\n",
    "\n",
    "print(\"Sample observation:\")\n",
    "print(sample_observation)\n",
    "print(\"observation shape: \", sample_obs[\"speed\"].shape, sample_obs[\"image\"].shape)\n",
    "\n",
    "# Forward pass\n",
    "custom_extractor = CustomCombinedExtractor(env.observation_space)\n",
    "feature_results = custom_extractor(sample_observation)\n",
    "\n",
    "print(\"\\nFeatures shape:\", feature_results.shape)\n",
    "\n",
    "print(\"feature dim: \", custom_extractor.features_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further define the network used, you can create a custom network. But I don't think it is necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "# Policy keyword arguments with optimized architecture\n",
    "policy_kwargs = {\n",
    "    'ortho_init': False,  # Orthogonal initialization may be beneficial\n",
    "    'activation_fn': nn.ReLU,  # Explicitly define activation function\n",
    "    'net_arch': {\n",
    "        'pi': [256, 256],  # Increased layer sizes for policy network\n",
    "        'vf': [256, 256],  # Increased layer sizes for value function\n",
    "    },\n",
    "    'features_extractor_class': CustomCombinedExtractor, # use the custom feature extractor\n",
    "    'features_extractor_kwargs': {},  #\n",
    "    # 'share_features_extractor': False,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# Define the PPO model with optimized parameters\n",
    "\n",
    "\n",
    "# change the parameters as needed, check this https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "model = PPO(\n",
    "    # CustomActorCriticPolicy,\n",
    "    \"MultiInputPolicy\",\n",
    "    venv,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./logs\",\n",
    "    \n",
    "    policy_kwargs=policy_kwargs,\n",
    "    # learning_rate=linear_schedule(3e-4), \n",
    "    learning_rate=1e-3,\n",
    "    n_steps=4000,  \n",
    "    batch_size=1000, \n",
    "    n_epochs=6,  \n",
    "    gamma=0.999,  \n",
    "    gae_lambda=0.95,  \n",
    "    clip_range=0.2,  \n",
    "    ent_coef=0.01, \n",
    "    vf_coef=0.5,  \n",
    "    max_grad_norm = 100.0, \n",
    "    # target_kl=0.015,\n",
    "\n",
    "    device=\"auto\",  # Use GPU if available\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibble approach is to use PPO with LSTM. LSTM should be useful for this application. But.....will require more vram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sb3_contrib import RecurrentPPO\n",
    "\n",
    "\n",
    "# policy_kwargs = {\n",
    "#     'ortho_init': False,  # Orthogonal initialization can be beneficial, consider enabling\n",
    "#     'activation_fn': nn.ReLU,  # Explicitly define activation function\n",
    "#     'net_arch': {\n",
    "#         # 'shared_extractor': None,  # Using custom feature extractor\n",
    "#         'pi': [256, 256],  # Increased layer sizes for policy network\n",
    "#         'vf': [256, 256],  # Increased layer sizes for value function\n",
    "#     },\n",
    "#     'features_extractor_class': CustomCombinedExtractor,\n",
    "#     'features_extractor_kwargs': {},  # Add if any additional args needed\n",
    "#     # 'share_features_extractor': False,  # Ensure features extractor is not shared\n",
    "#     'n_lstm_layers': 15,\n",
    "    \n",
    "# }\n",
    "\n",
    "# model = RecurrentPPO(\n",
    "#     \"MlpLstmPolicy\",\n",
    "#     venv,\n",
    "#     verbose=1,\n",
    "#     tensorboard_log=\"./logs\",\n",
    "\n",
    "#     policy_kwargs=policy_kwargs,\n",
    "    \n",
    "#     learning_rate=3e-4,\n",
    "#     n_steps=3000,\n",
    "#     batch_size=500,\n",
    "#     n_epochs=4,\n",
    "#     gamma=0.9992,\n",
    "#     gae_lambda=0.95,\n",
    "#     clip_range=0.2,\n",
    "#     ent_coef=0.01,\n",
    "#     vf_coef=0.5,\n",
    "#     max_grad_norm= 100, \n",
    "\n",
    "#     device=\"auto\"\n",
    "# )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "The code below shows how training is done using PPO. It also shows how to load and save models and venv.\n",
    "To train more, simply increase total_timesteps. You can change the tb_log_name to log different trainings.\n",
    "## Warning:\n",
    "- Do not load save unless it is exact same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_checkpoint = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = ['lighthouse', 'zengarden', 'hacienda', 'snowtuxpeak', 'cornfield_crossing', 'scotland', 'cocoa_temple']\n",
    "\n",
    "# use this code to train on different tracks\n",
    "env.changeTrack(\"lighthouse\")\n",
    "    \n",
    "\n",
    "if load_from_checkpoint and os.path.exists(\"ppo_kart.zip\"):\n",
    "    model = PPO.load(\"ppo_kart\", env=venv)\n",
    "    # model = RecurrentPPO.load(\"ppo_kart\", env=venv)\n",
    "    print(\"Model loaded from checkpoint\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this can log to tensorboard\n",
    "# reset_num_timesteps=False is used to continue training from the previous timesteps, this if for logging purposes\n",
    "# progress bar may not work well in some cases\n",
    "\n",
    "# significantly increase the total_timesteps for better performance\n",
    "model.learn(total_timesteps=2000, reset_num_timesteps=False, tb_log_name=\"ppo_test1\", log_interval=1, progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model and params to the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_kart\")\n",
    "venv.save(\"venv.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing the video\n",
    "The code below shows how to render and play video using the env.\n",
    "Note, our env use a different rendering logic than usual GYM envs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reset the environment and get the initial observation\n",
    "obs, info = env.reset()\n",
    "\n",
    "lap_completed = False\n",
    "truncated = False\n",
    "\n",
    "# Generate a video until the kart finishes the track or the episode is truncated\n",
    "while not (lap_completed or truncated):\n",
    "    print(\n",
    "        f\"Processing frame {env.currentFrame},lap_completed? {lap_completed or truncated}\",\n",
    "        end=\"\\r\"\n",
    "    )\n",
    "    \n",
    "    # Get the action from the trained model based on the current observation\n",
    "    action, _states = model.predict(obs)\n",
    "    \n",
    "    # Perform the action in the environment\n",
    "    obs, rewards, lap_completed, truncated, info = env.step(action)\n",
    "\n",
    "    # Render the current frame (collect frames for the video)\n",
    "    env.render()\n",
    "    \n",
    "# Play the generated video\n",
    "env.playVideo()\n",
    "\n",
    "# If you see \"finished at frame: 2000\", it means it timed out, not actually finished\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
